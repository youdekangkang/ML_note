Embedding层，又被称为嵌入层，可以是被认为一种向量表示实体的方法。这种技术源于自然语言处理被用于文本的表示，但是再在推荐系统中，它可以用来表示各种物品（这两种技术毕竟是相同的，本质上没有很大的区别）。一般来说，embedding层的表示做好了，基本上推荐系统的任务就完成了一大半。



### 什么是word embedding

我们知道，任何数学计算任务都是基于数字的，所以对于各种实体我们也需要使用一种方式首先将他们转换为数字。事实上我已经有一种表示的方式，也就是One-Hot编码，它是使用位置矩阵来表示一段文本（物品），但是他的缺点也十分明显，当文本量非常大的时候，使用这种编码方式会使得维度增加得非常高，最终可能造成维度爆炸。

但是从独热编码本身来看，它使用了大量的0来表示位置，有很多的信息可以进行压缩。所以这个时候我们又想到了另外一种可以表示位置的方式，那就是向量。

这就引出了我们的embedding，它本质上就是一种稠密向量。使用它的核心思想其实就是：**相似的词汇（物品）往往都是出现在同一环境中的（例如<u>眼睛</u>可能会出现在<u>检查</u>这个词的附近）。出现在非常相似的分布（其相邻的词是相似的）中给的两个词具有相似的含义。**所以这也是为什么我们可以把它同样应用于推荐系统中，或者说，推荐系统就是应用这一种原理进行推荐。

![onehot](https://pic1.zhimg.com/80/v2-1ded42011e9dd14893d7872074b808d8_720w.jpg)

任何 embedding 一开始都是一个随机数，然后随着优化算法，不断迭代更新，最后网络收敛停止迭代的时候，网络各个层的参数就相对固化，得到隐层权重表（此时就相当于得到了我们想要的 embedding），然后在通过查表可以单独查看每个元素的 embedding。

通过计算用户和物品的Embedding相似度，Embedding可以直接作为推荐系统的召回层或者召回策略之一（比如Youtube推荐模型等）。 Embedding对物品、用户相似度的计算是常用的推荐系统召回层技术。

这个表示的过程应该是这样：MF --> Sequence Embedding --> Grap Embedding

![产生embedding](https://pic2.zhimg.com/80/v2-22fd6e8ebecd09a234d17532a268ba6d_720w.jpg)



### 如何生成Embedding

**矩阵分解**

生成embedding的方法有很多，这里我们首先介绍矩阵分解这种最简单的方法：

这个方法是将关系矩阵分解为这两个东西的Embedding矩阵。例如，我们通过用户-物品关系矩阵能够获得用户与物品的embedding矩阵。

![](https://pic2.zhimg.com/80/v2-c3891da0437e97e2e098380b7f5415d1_720w.jpg)

 
