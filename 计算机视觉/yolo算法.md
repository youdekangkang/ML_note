# 各种YOLO算法

[Toc]

### Yolo v1

YOLO意思是You Only Look Once，创造性的将候选区和对象识别这两个阶段合二为一，看一眼图片（不用看两眼哦）就能知道有哪些对象以及它们的位置。

实际上，YOLO并没有真正去掉候选区，而是采用了预定义的候选区（准确点说应该是预测区，因为并不是Faster RCNN所采用的Anchor）。也就是将图片划分为 7*7=49 个网格（grid），每个网格允许预测出2个边框（bounding box，包含某个对象的矩形框），总共 49*2=98 个bounding box。可以理解为98个候选区，它们很粗略的覆盖了图片的整个区域。

### Yolo v2

![image-20220520141331288](yolo算法.assets/image-20220520141331288.png)

#### V2中的聚类提取先验框

faster-rcnn中的先验框比例一般都是常规固定的，很明显这是一种空间换时间的方法，而且不一定都能够适合所有的算法

yolo v2使用K-means聚类的方法找到最适合的anchor尺寸,使用定义的距离：
$$
d(bos,centroids) = 1 - IOU(box,centroids)
$$
下图展示了聚类的簇的个数和IOU之间的关系，两条曲线分别代表了VOC和COCO数据集的测试结果。最后结合不同的K值对召回率的影响，论文选择了K=5。右边的示意图是选出来的5个box的大小，这里紫色和黑色也是分别表示两个不同的数据集，可以看出其基本形状是类似的。而且发现聚类的结果和手动设置的anchor b ox大小差别显著。聚类的结果中多是高瘦的box，而矮胖的box数量较少，这也比较符合数据集中目标的视觉效果。

![image-20220520121145075](yolo算法.assets/image-20220520121145075.png)

#### 偏移量的计算

![image-20220520124825112](yolo算法.assets/image-20220520124825112.png)

![image-20220520125029066](yolo算法.assets/image-20220520125029066.png)

#### v2 特征融合

最后一层的感受野太大了，小目标可能会丢失，所以会融合之前的特征

 ![image-20220520140532609](yolo算法.assets/image-20220520140532609.png)



### Yolo v3

v3中最大的改进就是网络结构，使其更加适合进行小目标的检测

![image-20220520141458931](yolo算法.assets/image-20220520141458931.png)

Yolo v3采用三种不同尺寸的采样方式，例如假设图像的尺寸为416\*416，得到的特征映射为13\*13的大小，这里使用1\*1的检测核，为我们提供13\*13\*255的检测特征映射。接下来2倍上采样增加到26×26的维度。

13 x 13层负责检测大型目标对象，而52 x 52层检测较小的目标对象，26 x 26层检测中等大小目标对象。大概的融合流程如下：

![image-20220520150206275](yolo算法.assets/image-20220520150206275.png)



####  残差连接

这里的理论来源于Resnet的操作，虽然普遍认为神经网络模型越深，拟合能力越好，但是由于梯度消失/爆炸等问题，在深度到达了一定程度后, 模型的表现会不升反降。Resnet的结构也非常简单：

![image-20220524110255279](yolo算法.assets/image-20220524110255279.png)

相当于在输出下一层之前需要保持两种选择方式，如果F(x)的运行效果不如原来的x，那么就选择保持原来的x。虽然网络的效果可能不会有很大的提升，但是能够保证效果不必原来的差。



#### 核心网络架构图

![image-20220524112803182](yolo算法.assets/image-20220524112803182.png)

V3版本还能够预测多标签任务

在最终的一个tensor中可以使用阈值判断那些标签适合检测的目标



